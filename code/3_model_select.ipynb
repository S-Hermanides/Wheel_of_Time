{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:19:19.786126Z",
     "start_time": "2020-08-23T19:19:18.124840Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stephan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import NMF\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from data_extract_clean import stem_tokenizer\n",
    "from analysis_functions import display_topics\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:19:19.952323Z",
     "start_time": "2020-08-23T19:19:19.791535Z"
    }
   },
   "outputs": [],
   "source": [
    "chapters = pd.read_pickle('../data_files/chapter_corpus.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:19:19.971499Z",
     "start_time": "2020-08-23T19:19:19.956347Z"
    }
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['becaus', 'said'])\n",
    "porter_stemmer = PorterStemmer()\n",
    "stop_words_stemmed = [porter_stemmer.stem(word) for word in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling: compare algorithms and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using parameters from the tests in the notebook \"doc_term_matrix_size\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:43.012184Z",
     "start_time": "2020-08-23T19:19:19.976716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Chapters. Tf-IDF with stemming, min_df, max_df\n",
    "tf1 = TfidfVectorizer(stop_words=stop_words_stemmed, ngram_range=(1, 1), tokenizer=stem_tokenizer, min_df=2, max_df=0.9)\n",
    "chapters_tf1 = tf1.fit_transform(chapters)\n",
    "chapters_dtm = pd.DataFrame(chapters_tf1.toarray(), columns=tf1.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:43.688315Z",
     "start_time": "2020-08-23T19:20:43.014512Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00920405, 0.04420284, 0.03208759, 0.0279908 , 0.02507649,\n",
       "       0.01969872, 0.0178067 , 0.01569217, 0.01390436, 0.01276063,\n",
       "       0.01131965, 0.01012168, 0.00957697, 0.00955916, 0.00882688])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(15)\n",
    "doc_topic = lsa.fit_transform(chapters_dtm)\n",
    "lsa.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:43.705126Z",
     "start_time": "2020-08-23T19:20:43.692943Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26782868505682506"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lsa.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:43.762926Z",
     "start_time": "2020-08-23T19:20:43.708617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  0\n",
      "rand, egwen, perrin, nynaev, mat, elayn, moirain, aviendha, lan, min, siuan, lord, aiel, thom, trolloc\n",
      "\n",
      "Topic  1\n",
      "nynaev, elayn, egwen, siuan, birgitt, sheriam, amyrlin, elaida, aviendha, novic, tower, sister, moghedien, ajah, romanda\n",
      "\n",
      "Topic  2\n",
      "mat, thom, rand, tuon, bloodi, dice, noal, olver, tylin, talman, jolin, luca, egeanin, selucia, juilin\n",
      "\n",
      "Topic  3\n",
      "elayn, perrin, nynaev, birgitt, fail, thom, galad, juilin, mat, luca, gaul, berelain, reann, hopper, whitecloak\n",
      "\n",
      "Topic  4\n",
      "mat, siuan, egwen, tuon, thom, perrin, amyrlin, bryne, sheriam, elaida, talman, noal, sitter, lelain, bloodi\n",
      "\n",
      "Topic  5\n",
      "nynaev, loial, moirain, egwen, lan, rand, ingtar, hurin, perrin, thom, ogier, verin, trolloc, fain, selen\n",
      "\n",
      "Topic  6\n",
      "siuan, cadsuan, min, moirain, sister, merean, nynaev, tamra, room, loial, myrel, street, logain, ladi, lean\n",
      "\n",
      "Topic  7\n",
      "cadsuan, min, egwen, nynaev, tuon, gawyn, seanchan, verin, daman, galad, rand, mat, alanna, perrin, semirhag\n",
      "\n",
      "Topic  8\n",
      "elayn, gawyn, birgitt, trolloc, androl, loial, bryne, hurin, dyelin, ingtar, galad, iturald, rand, pevara, gateway\n",
      "\n",
      "Topic  9\n",
      "loial, hurin, ingtar, verin, selen, rand, lord, elayn, sheriam, elaida, ogier, berelain, fain, horn, amyrlin\n",
      "\n",
      "Topic  10\n",
      "tuon, egeanin, seanchan, lan, selucia, daman, suldam, domon, luca, trolloc, suroth, bukama, sevanna, ingtar, hurin\n",
      "\n",
      "Topic  11\n",
      "cadsuan, loial, aviendha, verin, ingtar, ogier, tuon, hurin, egwen, mat, olver, harin, sister, waygat, merilil\n",
      "\n",
      "Topic  12\n",
      "min, siuan, tuon, aviendha, bryne, gawyn, seanchan, loial, daman, selucia, suldam, egeanin, luca, rand, nynaev\n",
      "\n",
      "Topic  13\n",
      "androl, siuan, nynaev, pevara, aviendha, logain, taim, moghedien, slayer, hurin, sheriam, emarin, gaul, gateway, evin\n",
      "\n",
      "Topic  14\n",
      "gawyn, loial, hurin, sevanna, bryne, ingtar, cadsuan, nynaev, galad, galina, therava, wise, selen, thom, gaishain\n"
     ]
    }
   ],
   "source": [
    "display_topics(lsa, tf1.get_feature_names(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:43.799127Z",
     "start_time": "2020-08-23T19:20:43.766698Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.29825</td>\n",
       "      <td>-0.08435</td>\n",
       "      <td>0.00499</td>\n",
       "      <td>-0.04596</td>\n",
       "      <td>-0.01862</td>\n",
       "      <td>-0.03486</td>\n",
       "      <td>0.03860</td>\n",
       "      <td>-0.18299</td>\n",
       "      <td>-0.00795</td>\n",
       "      <td>-0.19560</td>\n",
       "      <td>0.22519</td>\n",
       "      <td>-0.00951</td>\n",
       "      <td>-0.00916</td>\n",
       "      <td>-0.15185</td>\n",
       "      <td>0.02623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26033</td>\n",
       "      <td>0.09163</td>\n",
       "      <td>-0.11884</td>\n",
       "      <td>-0.18856</td>\n",
       "      <td>0.14450</td>\n",
       "      <td>-0.01427</td>\n",
       "      <td>0.27197</td>\n",
       "      <td>-0.25863</td>\n",
       "      <td>-0.02397</td>\n",
       "      <td>-0.06780</td>\n",
       "      <td>-0.04258</td>\n",
       "      <td>0.01399</td>\n",
       "      <td>-0.02378</td>\n",
       "      <td>0.02102</td>\n",
       "      <td>-0.00441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.36504</td>\n",
       "      <td>0.13653</td>\n",
       "      <td>-0.18595</td>\n",
       "      <td>-0.25632</td>\n",
       "      <td>0.21412</td>\n",
       "      <td>-0.07637</td>\n",
       "      <td>0.40004</td>\n",
       "      <td>-0.31146</td>\n",
       "      <td>-0.02121</td>\n",
       "      <td>-0.05955</td>\n",
       "      <td>-0.08361</td>\n",
       "      <td>-0.00114</td>\n",
       "      <td>0.03205</td>\n",
       "      <td>0.13082</td>\n",
       "      <td>0.01102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.33236</td>\n",
       "      <td>0.06418</td>\n",
       "      <td>-0.10695</td>\n",
       "      <td>-0.16674</td>\n",
       "      <td>0.18094</td>\n",
       "      <td>-0.07357</td>\n",
       "      <td>0.33853</td>\n",
       "      <td>-0.24177</td>\n",
       "      <td>-0.00471</td>\n",
       "      <td>-0.01061</td>\n",
       "      <td>-0.02088</td>\n",
       "      <td>-0.01280</td>\n",
       "      <td>0.07003</td>\n",
       "      <td>0.05695</td>\n",
       "      <td>0.01918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.33638</td>\n",
       "      <td>0.08491</td>\n",
       "      <td>-0.14892</td>\n",
       "      <td>-0.22384</td>\n",
       "      <td>0.19916</td>\n",
       "      <td>-0.05446</td>\n",
       "      <td>0.37861</td>\n",
       "      <td>-0.34055</td>\n",
       "      <td>-0.03127</td>\n",
       "      <td>-0.08366</td>\n",
       "      <td>-0.06263</td>\n",
       "      <td>-0.00115</td>\n",
       "      <td>0.04313</td>\n",
       "      <td>0.04678</td>\n",
       "      <td>0.01258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>699</th>\n",
       "      <td>0.32942</td>\n",
       "      <td>-0.34041</td>\n",
       "      <td>-0.14458</td>\n",
       "      <td>0.19011</td>\n",
       "      <td>0.07913</td>\n",
       "      <td>0.06161</td>\n",
       "      <td>-0.11278</td>\n",
       "      <td>0.07322</td>\n",
       "      <td>0.00060</td>\n",
       "      <td>-0.12460</td>\n",
       "      <td>-0.11998</td>\n",
       "      <td>0.06262</td>\n",
       "      <td>0.01033</td>\n",
       "      <td>0.15952</td>\n",
       "      <td>-0.01684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.31001</td>\n",
       "      <td>-0.20433</td>\n",
       "      <td>-0.07296</td>\n",
       "      <td>0.04439</td>\n",
       "      <td>-0.11509</td>\n",
       "      <td>-0.07030</td>\n",
       "      <td>-0.12208</td>\n",
       "      <td>0.00911</td>\n",
       "      <td>0.04162</td>\n",
       "      <td>-0.10809</td>\n",
       "      <td>-0.06221</td>\n",
       "      <td>0.08851</td>\n",
       "      <td>-0.00100</td>\n",
       "      <td>0.21042</td>\n",
       "      <td>-0.00220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>0.22579</td>\n",
       "      <td>-0.00398</td>\n",
       "      <td>0.02998</td>\n",
       "      <td>-0.02122</td>\n",
       "      <td>-0.09334</td>\n",
       "      <td>-0.09871</td>\n",
       "      <td>-0.02736</td>\n",
       "      <td>-0.02420</td>\n",
       "      <td>0.12164</td>\n",
       "      <td>-0.11240</td>\n",
       "      <td>-0.00627</td>\n",
       "      <td>0.03522</td>\n",
       "      <td>-0.02136</td>\n",
       "      <td>0.21027</td>\n",
       "      <td>0.01544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>0.38527</td>\n",
       "      <td>-0.27729</td>\n",
       "      <td>-0.13089</td>\n",
       "      <td>0.04401</td>\n",
       "      <td>-0.09666</td>\n",
       "      <td>0.11449</td>\n",
       "      <td>-0.04039</td>\n",
       "      <td>0.03280</td>\n",
       "      <td>-0.03083</td>\n",
       "      <td>-0.08349</td>\n",
       "      <td>-0.06343</td>\n",
       "      <td>-0.08716</td>\n",
       "      <td>-0.02096</td>\n",
       "      <td>0.15128</td>\n",
       "      <td>-0.05095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>0.59529</td>\n",
       "      <td>-0.12458</td>\n",
       "      <td>0.02108</td>\n",
       "      <td>0.13628</td>\n",
       "      <td>-0.15315</td>\n",
       "      <td>0.04510</td>\n",
       "      <td>-0.04209</td>\n",
       "      <td>0.07009</td>\n",
       "      <td>0.08730</td>\n",
       "      <td>-0.08676</td>\n",
       "      <td>-0.09853</td>\n",
       "      <td>0.12556</td>\n",
       "      <td>0.14565</td>\n",
       "      <td>0.07324</td>\n",
       "      <td>-0.01840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>704 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0        1        2        3        4        5        6        7  \\\n",
       "0    0.29825 -0.08435  0.00499 -0.04596 -0.01862 -0.03486  0.03860 -0.18299   \n",
       "1    0.26033  0.09163 -0.11884 -0.18856  0.14450 -0.01427  0.27197 -0.25863   \n",
       "2    0.36504  0.13653 -0.18595 -0.25632  0.21412 -0.07637  0.40004 -0.31146   \n",
       "3    0.33236  0.06418 -0.10695 -0.16674  0.18094 -0.07357  0.33853 -0.24177   \n",
       "4    0.33638  0.08491 -0.14892 -0.22384  0.19916 -0.05446  0.37861 -0.34055   \n",
       "..       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "699  0.32942 -0.34041 -0.14458  0.19011  0.07913  0.06161 -0.11278  0.07322   \n",
       "700  0.31001 -0.20433 -0.07296  0.04439 -0.11509 -0.07030 -0.12208  0.00911   \n",
       "701  0.22579 -0.00398  0.02998 -0.02122 -0.09334 -0.09871 -0.02736 -0.02420   \n",
       "702  0.38527 -0.27729 -0.13089  0.04401 -0.09666  0.11449 -0.04039  0.03280   \n",
       "703  0.59529 -0.12458  0.02108  0.13628 -0.15315  0.04510 -0.04209  0.07009   \n",
       "\n",
       "           8        9       10       11       12       13       14  \n",
       "0   -0.00795 -0.19560  0.22519 -0.00951 -0.00916 -0.15185  0.02623  \n",
       "1   -0.02397 -0.06780 -0.04258  0.01399 -0.02378  0.02102 -0.00441  \n",
       "2   -0.02121 -0.05955 -0.08361 -0.00114  0.03205  0.13082  0.01102  \n",
       "3   -0.00471 -0.01061 -0.02088 -0.01280  0.07003  0.05695  0.01918  \n",
       "4   -0.03127 -0.08366 -0.06263 -0.00115  0.04313  0.04678  0.01258  \n",
       "..       ...      ...      ...      ...      ...      ...      ...  \n",
       "699  0.00060 -0.12460 -0.11998  0.06262  0.01033  0.15952 -0.01684  \n",
       "700  0.04162 -0.10809 -0.06221  0.08851 -0.00100  0.21042 -0.00220  \n",
       "701  0.12164 -0.11240 -0.00627  0.03522 -0.02136  0.21027  0.01544  \n",
       "702 -0.03083 -0.08349 -0.06343 -0.08716 -0.02096  0.15128 -0.05095  \n",
       "703  0.08730 -0.08676 -0.09853  0.12556  0.14565  0.07324 -0.01840  \n",
       "\n",
       "[704 rows x 15 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Vt = pd.DataFrame(doc_topic.round(5),\n",
    "             index = chapters.index)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impression**  \n",
    "There is a lot of overlap in the topics here, but it would be possible to name some of them based on the included words. I know from earlier tests that NMF comes up with better defined topics, so I am going to use that over LSA and compare to LDA below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.572294Z",
     "start_time": "2020-08-23T19:20:43.804560Z"
    }
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(15)\n",
    "doc_topic = nmf_model.fit_transform(chapters_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.605423Z",
     "start_time": "2020-08-23T19:20:48.589143Z"
    }
   },
   "outputs": [],
   "source": [
    "# Naming topics from NMF:\n",
    "topics = ['rand_main', 'Nynaeve', 'mat', 'perrin_wolfbrother', 'egwene_amyrlin', 'loial_rand', 'siuan_moiraine',\n",
    "         'aviendha_aiel', 'elayne_queen', 'moiraine_lan', 'tuon_seanchan', 'puppeteers_main', 'min_viewings', 'black_tower',\n",
    "         'gawyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.778758Z",
     "start_time": "2020-08-23T19:20:48.626079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic: ' rand_main '\n",
      "rand, therin, lew, basher, dragon, lord, tam, taim, saidin, maiden, sword, citi, aiel, weiramon, kill\n",
      "\n",
      "Topic: ' Nynaeve '\n",
      "nynaev, elayn, moghedien, thom, juilin, liandrin, luca, dream, telaranrhiod, door, channel, braid, galad, ship, tanchico\n",
      "\n",
      "Topic: ' mat '\n",
      "mat, thom, bloodi, talman, dice, olver, noal, gleeman, gholam, vanin, nalesean, cauthon, inn, mayb, tylin\n",
      "\n",
      "Topic: ' perrin_wolfbrother '\n",
      "perrin, fail, berelain, gaul, hopper, wolv, elya, masema, slayer, wolf, aram, galad, whitecloak, smell, lord\n",
      "\n",
      "Topic: ' egwene_amyrlin '\n",
      "egwen, amyrlin, sheriam, elaida, tower, verin, novic, sitter, ajah, silviana, romanda, hall, dream, sister, mother\n",
      "\n",
      "Topic: ' loial_rand '\n",
      "loial, hurin, rand, ingtar, ogier, verin, selen, horn, fain, lord, sted, waygat, trolloc, sniffer, perrin\n",
      "\n",
      "Topic: ' siuan_moiraine '\n",
      "siuan, moirain, sister, sheriam, bryne, elaida, myrel, amyrlin, lelain, tower, novic, romanda, sitter, lean, tamra\n",
      "\n",
      "Topic: ' aviendha_aiel '\n",
      "aviendha, aiel, wise, ami, rhuarc, maiden, bair, spear, chief, couladin, shaido, clan, melain, gaishain, rhuidean\n",
      "\n",
      "Topic: ' elayne_queen '\n",
      "elayn, birgitt, aviendha, dyelin, merilil, vanden, reann, norri, arymilla, alis, andor, windfind, queen, caemlyn, sister\n",
      "\n",
      "Topic: ' moiraine_lan '\n",
      "moirain, lan, trolloc, warder, rand, bukama, zarin, perrin, agelmar, blight, hors, sword, ryne, loial, night\n",
      "\n",
      "Topic: ' tuon_seanchan '\n",
      "tuon, egeanin, seanchan, selucia, luca, daman, suldam, domon, jolin, suroth, wagon, tylin, bethamin, teslyn, juilin\n",
      "\n",
      "Topic: ' puppeteers_main '\n",
      "cadsuan, harin, semirhag, sorilea, verin, alanna, meris, althor, shalon, saren, merana, daigian, sister, alivia, graendal\n",
      "\n",
      "Topic: ' min_viewings '\n",
      "min, rand, logain, caralin, merana, daman, seanchan, lean, suldam, renna, darlin, ship, aura, view, nynaev\n",
      "\n",
      "Topic: ' black_tower '\n",
      "androl, pevara, taim, logain, emarin, trolloc, gateway, evin, demandr, ashaman, jonneth, nalaam, sharan, talman, dobser\n",
      "\n",
      "Topic: ' gawyn '\n",
      "gawyn, bryne, galad, egwen, sleet, iturald, trolloc, soldier, elayn, armi, camp, fight, tower, forc, sharan\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_model, tf1.get_feature_names(), 15, topic_names=topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impressions**  \n",
    "These topics are clearly defined character arcs and can be used as such. Most topics start with the name of the central character(s) in that arc. NMF does a great job of separating them out. I will compare to LDA below.\n",
    "\n",
    "**Note:** Increasing the number of topics still yields clearly defined plots, but I consider them more minor ones (often spanning only a single book), so I am leaving them out of scope for the larger analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.802077Z",
     "start_time": "2020-08-23T19:20:48.791202Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_topics_df = pd.DataFrame(doc_topic, columns= topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-13T03:43:43.872448Z",
     "start_time": "2020-08-13T03:43:43.868203Z"
    }
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Gensim requires the doc term matrix to be in a slightly different format. I need to keep the sparse matrix (not a Dataframe), transpose it and create a dictionary with word positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.817559Z",
     "start_time": "2020-08-23T19:20:48.805223Z"
    }
   },
   "outputs": [],
   "source": [
    "# Take the sparse term-document matrix\n",
    "# Transpose it so the terms are the rows\n",
    "doc_word = chapters_tf1.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert to gensim\n",
    "We need to convert our sparse `scipy` matrix to a `gensim`-friendly object called a Corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.835983Z",
     "start_time": "2020-08-23T19:20:48.825275Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "corpus = matutils.Sparse2Corpus(doc_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Map matrix rows to words (tokens)\n",
    "We need to save a mapping (dict) of row id to word (token) for later use by gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:20:48.863222Z",
     "start_time": "2020-08-23T19:20:48.843411Z"
    }
   },
   "outputs": [],
   "source": [
    "id2word = dict((v, k) for k, v in tf1.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:26:50.304005Z",
     "start_time": "2020-08-23T19:20:48.869489Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create lda model\n",
    "lda = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:26:50.321347Z",
     "start_time": "2020-08-23T19:26:50.306983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.006*\"rand\" + 0.005*\"perrin\" + 0.004*\"egwen\" + 0.004*\"mat\" + 0.004*\"nynaev\" + 0.004*\"elayn\" + 0.003*\"moirain\" + 0.002*\"siuan\" + 0.002*\"aviendha\" + 0.002*\"min\"'),\n",
       " (1,\n",
       "  '0.002*\"merean\" + 0.001*\"edeyn\" + 0.001*\"almen\" + 0.001*\"bri\" + 0.001*\"larel\" + 0.001*\"isel\" + 0.001*\"diryk\" + 0.000*\"charn\" + 0.000*\"aesdaishar\" + 0.000*\"adan\"'),\n",
       " (2,\n",
       "  '0.000*\"mat\" + 0.000*\"rand\" + 0.000*\"thom\" + 0.000*\"egwen\" + 0.000*\"feran\" + 0.000*\"elayn\" + 0.000*\"barthan\" + 0.000*\"moirain\" + 0.000*\"nynaev\" + 0.000*\"lan\"'),\n",
       " (3,\n",
       "  '0.001*\"alei\" + 0.001*\"shielyn\" + 0.001*\"alseen\" + 0.000*\"jac\" + 0.000*\"jess\" + 0.000*\"dimana\" + 0.000*\"maril\" + 0.000*\"adelorna\" + 0.000*\"wil\" + 0.000*\"lewin\"'),\n",
       " (4,\n",
       "  '0.001*\"rolan\" + 0.001*\"lacil\" + 0.001*\"arrela\" + 0.001*\"chiad\" + 0.001*\"iralin\" + 0.001*\"jenn\" + 0.001*\"lewin\" + 0.001*\"bain\" + 0.001*\"zerah\" + 0.000*\"essand\"'),\n",
       " (5,\n",
       "  '0.002*\"darkhound\" + 0.001*\"garenia\" + 0.001*\"talaan\" + 0.001*\"ewin\" + 0.001*\"naeff\" + 0.001*\"cair\" + 0.001*\"mishrail\" + 0.001*\"barim\" + 0.001*\"boundless\" + 0.000*\"berowin\"'),\n",
       " (6,\n",
       "  '0.001*\"evin\" + 0.001*\"hake\" + 0.001*\"tamra\" + 0.001*\"habor\" + 0.001*\"aeldra\" + 0.000*\"keren\" + 0.000*\"kireyin\" + 0.000*\"norley\" + 0.000*\"gode\" + 0.000*\"mezar\"'),\n",
       " (7,\n",
       "  '0.001*\"covril\" + 0.001*\"comar\" + 0.001*\"tamor\" + 0.000*\"dena\" + 0.000*\"caldevwin\" + 0.000*\"turak\" + 0.000*\"silvi\" + 0.000*\"dormail\" + 0.000*\"liah\" + 0.000*\"zera\"'),\n",
       " (8,\n",
       "  '0.002*\"annoura\" + 0.001*\"gallenn\" + 0.001*\"merean\" + 0.001*\"tamra\" + 0.001*\"steler\" + 0.001*\"guenna\" + 0.001*\"ghealdanin\" + 0.001*\"meilyn\" + 0.001*\"tema\" + 0.001*\"cetalia\"'),\n",
       " (9,\n",
       "  '0.001*\"simion\" + 0.000*\"adelorna\" + 0.000*\"noam\" + 0.000*\"bunt\" + 0.000*\"rosil\" + 0.000*\"sixpoint\" + 0.000*\"egwen\" + 0.000*\"rand\" + 0.000*\"hasp\" + 0.000*\"jarra\"'),\n",
       " (10,\n",
       "  '0.001*\"caralin\" + 0.000*\"toram\" + 0.000*\"hightow\" + 0.000*\"hauler\" + 0.000*\"marcolin\" + 0.000*\"hopwil\" + 0.000*\"rand\" + 0.000*\"mat\" + 0.000*\"elayn\" + 0.000*\"perrin\"'),\n",
       " (11,\n",
       "  '0.001*\"zaida\" + 0.001*\"dobser\" + 0.000*\"essand\" + 0.000*\"sephani\" + 0.000*\"nari\" + 0.000*\"hark\" + 0.000*\"bakuvun\" + 0.000*\"chanel\" + 0.000*\"emarin\" + 0.000*\"rand\"'),\n",
       " (12,\n",
       "  '0.002*\"renna\" + 0.001*\"joiya\" + 0.001*\"hark\" + 0.001*\"beldein\" + 0.001*\"museng\" + 0.001*\"nakomi\" + 0.001*\"seeker\" + 0.001*\"ajimbura\" + 0.001*\"renald\" + 0.001*\"amico\"'),\n",
       " (13,\n",
       "  '0.001*\"bor\" + 0.001*\"luci\" + 0.000*\"catalyn\" + 0.000*\"conail\" + 0.000*\"kerb\" + 0.000*\"periv\" + 0.000*\"essand\" + 0.000*\"macura\" + 0.000*\"narenwin\" + 0.000*\"branlet\"'),\n",
       " (14,\n",
       "  '0.006*\"androl\" + 0.004*\"pevara\" + 0.003*\"selen\" + 0.003*\"sulin\" + 0.002*\"alis\" + 0.002*\"weiramon\" + 0.002*\"arymilla\" + 0.002*\"norri\" + 0.002*\"adelea\" + 0.002*\"chiad\"')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impressions**  \n",
    "Many of these topics don't make sense to me. \n",
    "\n",
    "**Note:** It seems like after using Tf-IDF, LDA gets fixated on names and non-English (fictional) nouns. Trying this again with a normal CountVectorizer yields the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:26:53.497153Z",
     "start_time": "2020-08-23T19:26:50.324734Z"
    }
   },
   "outputs": [],
   "source": [
    "cv1 = CountVectorizer(stop_words='english', ngram_range=(1, 1), min_df=2, max_df=0.9)\n",
    "chapters_cv1 = cv1.fit_transform(chapters)\n",
    "chapters_dtm_cv = pd.DataFrame(chapters_cv1.toarray(), columns=cv1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:26:53.512884Z",
     "start_time": "2020-08-23T19:26:53.500214Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_word = chapters_cv1.transpose()\n",
    "corpus = matutils.Sparse2Corpus(doc_word)\n",
    "id2word = dict((v, k) for k, v in cv1.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:39:43.261810Z",
     "start_time": "2020-08-23T19:26:53.516470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"rand\" + 0.018*\"mat\" + 0.009*\"thom\" + 0.005*\"room\" + 0.005*\"street\" + 0.005*\"inn\" + 0.005*\"cloak\" + 0.004*\"master\" + 0.004*\"door\" + 0.004*\"caemlyn\"'),\n",
       " (1,\n",
       "  '0.017*\"egwene\" + 0.017*\"rand\" + 0.016*\"aiel\" + 0.012*\"aviendha\" + 0.012*\"wise\" + 0.009*\"ones\" + 0.007*\"amys\" + 0.006*\"moiraine\" + 0.005*\"rhuarc\" + 0.004*\"maidens\"'),\n",
       " (2,\n",
       "  '0.024*\"egwene\" + 0.022*\"siuan\" + 0.012*\"gawyn\" + 0.009*\"bryne\" + 0.007*\"tower\" + 0.006*\"amyrlin\" + 0.006*\"sheriam\" + 0.005*\"lelaine\" + 0.005*\"romanda\" + 0.005*\"sisters\"'),\n",
       " (3,\n",
       "  '0.017*\"rand\" + 0.006*\"lan\" + 0.006*\"moiraine\" + 0.004*\"stone\" + 0.004*\"sword\" + 0.004*\"perrin\" + 0.003*\"power\" + 0.003*\"black\" + 0.003*\"dead\" + 0.003*\"dragon\"'),\n",
       " (4,\n",
       "  '0.016*\"moiraine\" + 0.011*\"nynaeve\" + 0.009*\"siuan\" + 0.005*\"moghedien\" + 0.005*\"tower\" + 0.003*\"room\" + 0.003*\"accepted\" + 0.003*\"lan\" + 0.003*\"blue\" + 0.003*\"day\"'),\n",
       " (5,\n",
       "  '0.019*\"rand\" + 0.010*\"loial\" + 0.010*\"perrin\" + 0.006*\"trollocs\" + 0.005*\"ogier\" + 0.005*\"lord\" + 0.005*\"mat\" + 0.004*\"moiraine\" + 0.004*\"verin\" + 0.003*\"tam\"'),\n",
       " (6,\n",
       "  '0.023*\"egwene\" + 0.014*\"tower\" + 0.012*\"elaida\" + 0.010*\"amyrlin\" + 0.006*\"sisters\" + 0.006*\"ajah\" + 0.005*\"black\" + 0.004*\"red\" + 0.004*\"mother\" + 0.004*\"sister\"'),\n",
       " (7,\n",
       "  '0.007*\"rand\" + 0.006*\"elayne\" + 0.004*\"cadsuane\" + 0.004*\"aviendha\" + 0.003*\"min\" + 0.003*\"dragon\" + 0.003*\"lord\" + 0.003*\"city\" + 0.002*\"palace\" + 0.002*\"great\"'),\n",
       " (8,\n",
       "  '0.010*\"elyas\" + 0.007*\"perrin\" + 0.004*\"bukama\" + 0.003*\"raen\" + 0.003*\"children\" + 0.003*\"aram\" + 0.003*\"camp\" + 0.003*\"morgase\" + 0.003*\"suroth\" + 0.003*\"valda\"'),\n",
       " (9,\n",
       "  '0.036*\"nynaeve\" + 0.034*\"elayne\" + 0.006*\"birgitte\" + 0.006*\"egwene\" + 0.003*\"elaynes\" + 0.003*\"nynaeves\" + 0.003*\"thom\" + 0.003*\"door\" + 0.003*\"things\" + 0.003*\"room\"'),\n",
       " (10,\n",
       "  '0.030*\"min\" + 0.014*\"rand\" + 0.008*\"merana\" + 0.008*\"lord\" + 0.007*\"morgase\" + 0.006*\"alanna\" + 0.005*\"kiruna\" + 0.004*\"bera\" + 0.004*\"dragon\" + 0.004*\"young\"'),\n",
       " (11,\n",
       "  '0.039*\"perrin\" + 0.019*\"faile\" + 0.006*\"lord\" + 0.005*\"berelain\" + 0.004*\"wise\" + 0.004*\"aiel\" + 0.004*\"shaido\" + 0.004*\"ones\" + 0.004*\"camp\" + 0.003*\"perrins\"'),\n",
       " (12,\n",
       "  '0.029*\"mat\" + 0.007*\"thom\" + 0.005*\"bloody\" + 0.005*\"tuon\" + 0.004*\"seanchan\" + 0.003*\"dice\" + 0.003*\"maybe\" + 0.003*\"lord\" + 0.003*\"coat\" + 0.003*\"sure\"'),\n",
       " (13,\n",
       "  '0.009*\"rand\" + 0.005*\"asked\" + 0.005*\"mat\" + 0.005*\"trollocs\" + 0.004*\"battle\" + 0.004*\"elayne\" + 0.004*\"air\" + 0.003*\"fight\" + 0.003*\"place\" + 0.003*\"ground\"'),\n",
       " (14,\n",
       "  '0.020*\"damane\" + 0.019*\"seanchan\" + 0.016*\"suldam\" + 0.014*\"min\" + 0.009*\"egwene\" + 0.008*\"nynaeve\" + 0.008*\"falme\" + 0.006*\"suroth\" + 0.006*\"rand\" + 0.006*\"high\"')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus=corpus, num_topics=15, id2word=id2word, passes=50)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impressions**  \n",
    "These topics are starting to look like the ones NMF came up with, but there still some that have no clearly defined theme or topic that stands out. For instance the first three don't give a clear topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will stick with the NMF results using Tf-IDF without bi-grams for the rest of my analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:50:53.735610Z",
     "start_time": "2020-08-23T19:50:53.275406Z"
    }
   },
   "outputs": [],
   "source": [
    "doc_topics_df['clean_text'] = chapters\n",
    "\n",
    "metadata = pd.read_pickle('../data_files/extracted_text.pickle')\n",
    "\n",
    "combined = metadata.merge(doc_topics_df, how='left', on='clean_text')\n",
    "\n",
    "combined['word_count'] = combined['clean_text'].str.count(' ') + 1\n",
    "\n",
    "combined.to_pickle('../data_files/result.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:50:55.535901Z",
     "start_time": "2020-08-23T19:50:55.525227Z"
    }
   },
   "outputs": [],
   "source": [
    "columns = ['book_title', 'book_nr', 'chapter_title', 'chapter_nr', 'word_count', \n",
    "           'rand_main', 'Nynaeve', 'mat', 'perrin_wolfbrother', 'egwene_amyrlin', 'loial_rand', \n",
    "           'siuan_moiraine', 'aviendha_aiel', 'elayne_queen', 'moiraine_lan', 'tuon_seanchan', 'puppeteers_main', \n",
    "           'min_viewings', 'black_tower', 'gawyn']\n",
    "for_viz = combined[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-23T19:50:56.911779Z",
     "start_time": "2020-08-23T19:50:56.879936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output to csv for use in Tableau visualization\n",
    "for_viz.to_csv('../data_files/for_viz.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
